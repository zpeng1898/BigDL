(py378) [kai@Almaren-Node-107 xshards]$ python pytorch_train_xshards.py --data_dir hdfs://172.16.0.105:8020/user/kai/pzy/data/NCF/ml-1m  --cluster_mode yarn-client
Initializing orca context
Current pyspark location is : /home/kai/anaconda3/envs/py378/lib/python3.7/site-packages/pyspark/__init__.py
Initializing SparkContext for yarn-client mode
Start to pack current python env
Collecting packages...
Packing environment at '/home/kai/anaconda3/envs/py378' to '/tmp/tmps59p6wez/python_env.tar.gz'
[########################################] | 100% Completed | 36.9s
Packing has been completed: /tmp/tmps59p6wez/python_env.tar.gz
pyspark_submit_args is: --master yarn --deploy-mode client --archives /tmp/tmps59p6wez/python_env.tar.gz#python_env --driver-cores 4 --driver-memory 2g --num-executors 2 --executor-cores 4 --executor-memory 10g --py-files pytorch_model.py --driver-class-path /home/kai/anaconda3/envs/py378/lib/python3.7/site-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_3.1.3-2.2.0-SNAPSHOT-jar-with-dependencies.jar:/home/kai/anaconda3/envs/py378/lib/python3.7/site-packages/bigdl/share/core/lib/all-2.2.0-20221215.235822-10.jar:/home/kai/anaconda3/envs/py378/lib/python3.7/site-packages/bigdl/share/orca/lib/bigdl-orca-spark_3.1.3-2.2.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2023-01-14 16:25:17 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-01-14 16:25:18 WARN  Client:69 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.

2023-01-14 16:28:19,042 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:19,046 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:19,046 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:19,047 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
23-01-14 16:28:19 [Thread-5] INFO  Engine$:122 - Auto detect executor number and executor cores number
23-01-14 16:28:19 [Thread-5] INFO  Engine$:124 - Executor number is 2 and executor cores number is 4

User settings:

   KMP_AFFINITY=granularity=fine,compact,1,0
   KMP_BLOCKTIME=0
   KMP_DUPLICATE_LIB_OK=True
   KMP_INIT_AT_FORK=FALSE
   KMP_SETTINGS=1
   OMP_NUM_THREADS=1

Effective settings:

   KMP_ABORT_DELAY=0
   KMP_ADAPTIVE_LOCK_PROPS='1,1024'
   KMP_ALIGN_ALLOC=64
   KMP_ALL_THREADPRIVATE=352
   KMP_ATOMIC_MODE=2
   KMP_BLOCKTIME=0
   KMP_CPUINFO_FILE: value is not defined
   KMP_DETERMINISTIC_REDUCTION=false
   KMP_DEVICE_THREAD_LIMIT=2147483647
   KMP_DISP_HAND_THREAD=false
   KMP_DISP_NUM_BUFFERS=7
   KMP_DUPLICATE_LIB_OK=true
   KMP_FORCE_REDUCTION: value is not defined
   KMP_FOREIGN_THREADS_THREADPRIVATE=true
   KMP_FORKJOIN_BARRIER='2,2'
   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'
   KMP_FORKJOIN_FRAMES=true
   KMP_FORKJOIN_FRAMES_MODE=3
   KMP_GTID_MODE=3
   KMP_HANDLE_SIGNALS=false
   KMP_HOT_TEAMS_MAX_LEVEL=1
   KMP_HOT_TEAMS_MODE=0
   KMP_INIT_AT_FORK=true
   KMP_INIT_WAIT=2048
   KMP_ITT_PREPARE_DELAY=0
   KMP_LIBRARY=throughput
   KMP_LOCK_KIND=queuing
   KMP_MALLOC_POOL_INCR=1M
   KMP_NEXT_WAIT=1024
   KMP_NUM_LOCKS_IN_BLOCK=1
   KMP_PLAIN_BARRIER='2,2'
   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'
   KMP_REDUCTION_BARRIER='1,1'
   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'
   KMP_SCHEDULE='static,balanced;guided,iterative'
   KMP_SETTINGS=true
   KMP_SPIN_BACKOFF_PARAMS='4096,100'
   KMP_STACKOFFSET=64
   KMP_STACKPAD=0
   KMP_STACKSIZE=4M
   KMP_STORAGE_MAP=false
   KMP_TASKING=2
   KMP_TASKLOOP_MIN_TASKS=0
   KMP_TASK_STEALING_CONSTRAINT=1
   KMP_TEAMS_THREAD_LIMIT=88
   KMP_TOPOLOGY_METHOD=all
   KMP_USER_LEVEL_MWAIT=false
   KMP_VERSION=false
   KMP_WARNINGS=true
   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'
   OMP_ALLOCATOR=omp_default_mem_alloc
   OMP_CANCELLATION=false
   OMP_DEFAULT_DEVICE=0
   OMP_DISPLAY_AFFINITY=false
   OMP_DISPLAY_ENV=false
   OMP_DYNAMIC=false
   OMP_MAX_ACTIVE_LEVELS=2147483647
   OMP_MAX_TASK_PRIORITY=0
   OMP_NESTED=false
   OMP_NUM_THREADS='1'
   OMP_PLACES: value is not defined
   OMP_PROC_BIND='intel'
   OMP_SCHEDULE='static'
   OMP_STACKSIZE=4M
   OMP_TARGET_OFFLOAD=DEFAULT
   OMP_THREAD_LIMIT=2147483647
   OMP_TOOL=enabled
   OMP_TOOL_LIBRARIES: value is not defined
   OMP_WAIT_POLICY=PASSIVE
   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'

23-01-14 16:28:19 [Thread-5] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 31
2023-01-14 16:28:19 WARN  SparkContext:69 - Using an existing SparkContext; some configuration may not take effect.
23-01-14 16:28:19 [Thread-5] INFO  Engine$:461 - Find existing spark context. Checking the spark conf...
cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.Sample
BigDLBasePickler registering: bigdl.dllib.utils.common  Sample
cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.EvaluatedResult
BigDLBasePickler registering: bigdl.dllib.utils.common  EvaluatedResult
cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JTensor
BigDLBasePickler registering: bigdl.dllib.utils.common  JTensor
cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JActivity
BigDLBasePickler registering: bigdl.dllib.utils.common  JActivity
Loading data...
2023-01-14 16:28:20,027 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:20,029 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:20,032 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:20,033 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
23-01-14 16:28:20 [Thread-5] INFO  Engine$:122 - Auto detect executor number and executor cores number
23-01-14 16:28:20 [Thread-5] INFO  Engine$:124 - Executor number is 2 and executor cores number is 4
23-01-14 16:28:20 [Thread-5] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 31
2023-01-14 16:28:20 WARN  SparkContext:69 - Using an existing SparkContext; some configuration may not take effect.
23-01-14 16:28:20 [Thread-5] INFO  Engine$:461 - Find existing spark context. Checking the spark conf...
2023-01-14 16:28:34,599 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:34,600 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:34,602 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:34,604 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
23-01-14 16:28:34 [Thread-5] INFO  Engine$:122 - Auto detect executor number and executor cores number
23-01-14 16:28:34 [Thread-5] INFO  Engine$:124 - Executor number is 2 and executor cores number is 4
23-01-14 16:28:34 [Thread-5] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 31
2023-01-14 16:28:34 WARN  SparkContext:69 - Using an existing SparkContext; some configuration may not take effect.
23-01-14 16:28:34 [Thread-5] INFO  Engine$:461 - Find existing spark context. Checking the spark conf...
2023-01-14 16:28:37,095 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:37,096 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:37,097 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
2023-01-14 16:28:37,098 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false
23-01-14 16:28:37 [Thread-5] INFO  Engine$:122 - Auto detect executor number and executor cores number
23-01-14 16:28:37 [Thread-5] INFO  Engine$:124 - Executor number is 2 and executor cores number is 4
23-01-14 16:28:37 [Thread-5] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 31
2023-01-14 16:28:37 WARN  SparkContext:69 - Using an existing SparkContext; some configuration may not take effect.
23-01-14 16:28:37 [Thread-5] INFO  Engine$:461 - Find existing spark context. Checking the spark conf...
Processing features...
Try to unpersist an uncached rdd                                                
Negative sampling...                                                            
2023-01-14 16:29:04 WARN  TaskSetManager:69 - Lost task 4.0 in stage 87.0 (TID 1883) (Almaren-Node-118 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/disk3/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000002/pyspark.zip/pyspark/worker.py", line 586, in main
    func, profiler, deserializer, serializer = read_command(pickleSer, infile)
  File "/disk3/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000002/pyspark.zip/pyspark/worker.py", line 69, in read_command
    command = serializer._read_with_length(file)
  File "/disk3/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000002/pyspark.zip/pyspark/serializers.py", line 160, in _read_with_length
    return self.loads(obj)
  File "/disk3/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000002/pyspark.zip/pyspark/serializers.py", line 430, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'process_xshards'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1440)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2023-01-14 16:29:04 WARN  TaskSetManager:69 - Lost task 1.0 in stage 87.0 (TID 1880) (Almaren-Node-118 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/disk4/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000003/pyspark.zip/pyspark/worker.py", line 586, in main
    func, profiler, deserializer, serializer = read_command(pickleSer, infile)
  File "/disk4/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000003/pyspark.zip/pyspark/worker.py", line 69, in read_command
    command = serializer._read_with_length(file)
  File "/disk4/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000003/pyspark.zip/pyspark/serializers.py", line 160, in _read_with_length
    return self.loads(obj)
  File "/disk4/yarn/nm/usercache/root/appcache/application_1668477395550_0484/container_1668477395550_0484_01_000003/pyspark.zip/pyspark/serializers.py", line 430, in loads
    return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'process_xshards'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:349)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1440)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)
